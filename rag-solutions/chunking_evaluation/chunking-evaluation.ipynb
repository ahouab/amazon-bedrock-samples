{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0edf715-add1-4ba2-b278-6a0eb9c69aa1",
   "metadata": {},
   "source": [
    "<h1> Evaluating different chunking strategies, chunk sizing and overlap using RAGA framework: Claude 3 Sonnet on Bedrock for LLM, FAISS for Vector Store and RAGA for Evaluation   <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a7ac5e-252d-42fd-80cb-dd8de2a61150",
   "metadata": {},
   "source": [
    "#### Identifying  documents best chunking Strategy is key to increase the accuracy of retrieved responses with solutions involving RAG and LLM models. The following notebooks implements a framework to evaluate different chuncking strategies. The goal is not to identify the best chunking strategy in the context of the example from the notebook rather to provide a tool to implement this evaluation for real case example \n",
    "The notebook follow the following steps\n",
    "1. Install required libraries\n",
    "2. Create a bedrock Client\n",
    "3. Load documents example in a local folder\n",
    "4. Use a custom function \"textsplitterStrategy\" with 3 params : Strategy , chunk size and overlap\n",
    "5. run the function for 5 chunking strategies and load the documents in FAISS (5 stores)\n",
    "6. Use Claude3 Sonnet and RAGA to evaluate  context_recall,context_precision,  answer_relevancy\n",
    "7. plot the evaluation results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cecf6-fedf-4b6f-8460-c537bcec72ef",
   "metadata": {},
   "source": [
    "<h4>Install required libraries<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d3a5f-c6b0-446b-bbab-4e60a5718c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3  install  -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0f59c-739c-4833-8dc0-024b3a9461a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fec763-e2bf-44ee-8f63-4ecf98ac545e",
   "metadata": {},
   "source": [
    "<h4>Create a bedrock client using boto3<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545d396-af93-470f-9e30-43cd8cc6308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Create a bedrock Client\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import pprint\n",
    "import random\n",
    "from retrying import retry\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region_name,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33ebde-5277-415f-a5e7-d3acbd69cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2022, source=filenames[0]),\n",
    "    dict(year=2021, source=filenames[1]),\n",
    "    dict(year=2020, source=filenames[2]),\n",
    "    dict(year=2019, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcba01-542d-4cbc-b511-24be2c32a2ca",
   "metadata": {},
   "source": [
    "<h3> Use a custom function \"textsplitterStrategy\" with 3 params : Strategy , chunk size and overlap</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70102569-7d9f-4cdb-a009-b6796495563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use a custom function \"textsplitterStrategy\" with 3 params : Strategy , chunk size and overlap\n",
    "#textsplitter function : \n",
    "# param 1 : strategy   (LatexTextSplitter== 2 ), (NLTKTextSplitter== 1) ,(MarkdownTextSplitter' == 3)  , (RecursiveCharacterTextSplitter == 4),  strategytext='SpacyTextSplitter'\n",
    "#param 2: chunksize \n",
    "#param 3: overlapp\n",
    "\n",
    "#reminder of each Strategy \n",
    "\n",
    " # Markdown: Markdown is a lightweight markup language commonly used for formatting text. By recognizing the Markdown syntax (e.g., headings, lists, and code blocks), \n",
    " # Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators.\n",
    " #LaTex: LaTeX is a document preparation system and markup language often used for academic papers and technical documents. chunks respects the logical organization of the content (e.g., sections, subsections, and equations\n",
    " # NLTK is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It can help simplify textual data and gain in-depth information from input messages.\n",
    "# Spacy, another powerful NLP library, provides a sentence tokenization function that relies heavily on linguistic rules. It is a similar approach to NLTK.\n",
    "\n",
    "def textsplitterStrategy(strategy, chunksize , overlap):\n",
    "    from langchain.text_splitter import LatexTextSplitter\n",
    "    from langchain.text_splitter import MarkdownTextSplitter\n",
    "    from langchain.text_splitter import SpacyTextSplitter\n",
    "    from langchain.text_splitter import NLTKTextSplitter\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    documents = []\n",
    "    docs =[]\n",
    "\n",
    "    for idx, file in enumerate(filenames):\n",
    "        loader = PyPDFLoader(data_root + file)\n",
    "        document = loader.load()\n",
    "        for document_fragment in document:\n",
    "            document_fragment.metadata = metadata[idx]\n",
    "        \n",
    "        documents += document\n",
    "\n",
    "    if (strategy == 2):\n",
    "        text_splitter = LatexTextSplitter(\n",
    "            chunk_size =  chunksize,\n",
    "            chunk_overlap  = overlap,\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "  \n",
    "    if (strategy == 1):\n",
    "        text_splitter = NLTKTextSplitter(\n",
    "            chunk_size =  chunksize,\n",
    "            chunk_overlap  = overlap,\n",
    "         \n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents) \n",
    "    if (strategy == 3):\n",
    "        text_splitter = MarkdownTextSplitter(\n",
    "            chunk_size =  chunksize,\n",
    "            chunk_overlap  = overlap,\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "    if (strategy == 4):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size =  chunksize,\n",
    "            chunk_overlap  = overlap,\n",
    "           # is_separator_regex=False,\n",
    "            separators=[\"\\n\\n\", \"\\n\", ' ', ''],\n",
    "            length_function = len\n",
    "            \n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)  \n",
    "    if (strategy == 5):\n",
    "        text_splitter = SpacyTextSplitter(\n",
    "            chunk_size =  chunksize,\n",
    "            chunk_overlap  = overlap\n",
    "           #separator = ['\\n', '=', ',', '&']\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents) \n",
    "    avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "    print(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\n",
    "    print(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\n",
    "    print(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')         \n",
    "    return docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf942ad-9728-4eab-ba2c-6d8d266518ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get stragtey label to dispaly when plotting\n",
    "def Strategy_label(strategy):\n",
    "    strategytext=\"\"\n",
    "    if (strategy == 2):\n",
    "        strategytext='LatexTextSplitter'\n",
    "    if (strategy == 1):\n",
    "       strategytext='NLTKTextSplitter'\n",
    "    if (strategy == 3):\n",
    "          strategytext='MarkdownTextSplitter'\n",
    "    if (strategy == 4):\n",
    "        strategytext='RecursiveCharacterTextSplitter'\n",
    "    if (strategy == 5):\n",
    "        strategytext='SpacyTextSplitter'\n",
    "    return strategytext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7d476-609b-4608-b37e-73b213184051",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3   -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28149e3-43d6-4f99-b779-d0150a18e531",
   "metadata": {},
   "source": [
    "### Provide as parameter one of strategies below : and load chunk in a list of chunked documents\n",
    "#### 1 : NLTKTextSplitter\n",
    "#### 2 :LatexTextSplitter\n",
    "#### 3:MarkdownTextSplitter\n",
    "#### 4:RecursiveCharacterTextSplitter\n",
    "#### 5:SpacyTextSplitter\n",
    "#### Provide chunk size and overlap for docs with different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99eb94-776b-40ad-b4ec-521b10fc0ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Provide parameter one of those strategies: and load chunk in a list fo chunked documents\n",
    "#1: NLTKTextSplitter\n",
    "#2: LatexTextSplitter\n",
    "#3: MarkdownTextSplitter\n",
    "#4: RecursiveCharacterTextSplitter\n",
    "#5: SpacyTextSplitter\n",
    "#vectors for docs with different strategies\n",
    "#  chunk size as paramter : 1000 here for example purpose\n",
    "#  pass overlap as paramter : 150 here for example purpose  \n",
    "my_docs = []\n",
    "my_strategies =[]\n",
    "strategy=\"\"\n",
    "strategytext=\"\"\n",
    "for x in [1,2,3,4,5]:\n",
    "    strategy=Strategy_label(x)  \n",
    "    print (strategy)\n",
    "    prefixed_name = \"docs_\" + str(x)\n",
    "    prefixed_name =  textsplitterStrategy(x,1000,150)\n",
    "    my_docs.append(prefixed_name)\n",
    "    my_strategies.append(strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e3807-3abc-4f05-b4a9-969e605b77e3",
   "metadata": {},
   "source": [
    "### Bedrock embedding model : amazon.titan-embed-text-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd543a3-5c13-42da-92b8-592cf918d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', \n",
    "                              region_name='us-east-1')\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",\n",
    "                                       client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6f43c-9dc7-4450-9a8f-bd6a724f45ca",
   "metadata": {},
   "source": [
    "#### Run the function for 5 chunking strategies and load the documents in FAISS (5 stores)\n",
    "#### the execution of this loop  would requires 2-3 minutes to embed chunks and load them as vector in FAISS\n",
    "#### 5 strategies , 5 chunked documents (same documents chunked using different strategies) and loaded to FAISS: One store per Strategy¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa41b17-9052-4dd1-8498-e5628848058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS \n",
    "db_vector =[]\n",
    "for x in [1,2,3,4,5]:\n",
    "    db  = FAISS.from_documents(\n",
    "        my_docs[x-1],\n",
    "        bedrock_embeddings,\n",
    "    )\n",
    "    db_vector.append(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2939cb-b3d9-4d3f-968a-a0465009718a",
   "metadata": {},
   "source": [
    "#### Test the Vector Stores : use db_vector[x] , where x = 0,1,2,3,4\n",
    "##### 1: NLTKTextSplitter\n",
    "##### 2: LatexTextSplitter\n",
    "##### 3: MarkdownTextSplitter\n",
    "##### 4: RecursiveCharacterTextSplitter\n",
    "##### 5: SpacyTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbee6e7-7184-49d1-be71-5249de0a4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on the list of DBs\n",
    "query = \"What is Amazon doing in the field of Generative AI?\"\n",
    "retriever = db_vector[2].as_retriever()\n",
    "retrievalResults  = retriever.invoke(query)\n",
    "print(retrievalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1e5df-3d42-4821-b4b9-c287b44d9b39",
   "metadata": {},
   "source": [
    "#### Define LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c6307-242b-4f91-ab53-ceef555272de",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "#LLM prompt\n",
    "prompt = f\"\"\"\n",
    "Human: You are an advisor AI system, and provides answers to questions. \n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific.\n",
    "\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937388a7-385c-46e5-9f21-630797297a40",
   "metadata": {},
   "source": [
    "#### Define LLM paramters including max_tokens size, temperature and top-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d749e2a-6626-4428-bb1c-735bb4e7aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# payload with model paramters\n",
    "messages=[{ \"role\":'user', \"content\":[{'type':'text','text': prompt.format(contexts, query)}]}]\n",
    "sonnet_payload = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 2000,\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 1\n",
    "        }  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695c4f0-b9ca-4414-a672-52b12ebe2bfe",
   "metadata": {},
   "source": [
    "#### install boto3 required version , uncomment if errors only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b6053-4080-4240-bbe9-023cba1bc78a",
   "metadata": {},
   "source": [
    "#### Define LLM used for test : on our case model used for test anthropic.claude-3-sonnet-20240229-v1:0 on Bedrock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f7da0-c8f2-4716-8f14-1b07327b63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "response = bedrock_client.invoke_model(body=sonnet_payload, modelId=modelId, accept=accept, contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "response_text = response_body.get('content')[0]['text']\n",
    "llm = ChatBedrock(model_id=modelId, \n",
    "                  client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dec032-3715-4b49-baca-4e50a05f7f2c",
   "metadata": {},
   "source": [
    "#### Define RAG Chain evaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7242758-6c79-4e64-b3e4-bf670840f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define RAG Chain evaluator \n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate ,PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "    Human: You are an advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt2 = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50120174-2ff1-4e51-b6c2-7f9fa7a6f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pydantic==1.10.17 --force-reinstall  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9a46a-e948-43ef-8a87-68b66a700ac0",
   "metadata": {},
   "source": [
    "### Define RAGAS metrics LLM  : Context_recall, context_precision, answer_relevancy\n",
    "\n",
    "##### Context_recall : Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "##### Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks.\n",
    "\n",
    "##### Answer Correctness¶ : The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.\n",
    "\n",
    "###### https://docs.ragas.io/en/latest/concepts/metrics/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b43251-b1fa-4574-9437-1eeb4d25941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    answer_relevancy\n",
    "\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import harmfulness\n",
    "# list of metrics we're going to use\n",
    "metrics = [\n",
    "   # faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "  #  harmfulness,\n",
    "    answer_relevancy\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c2cc1-0690-4b69-92cb-6a8adb2f4028",
   "metadata": {},
   "source": [
    "#### This function will be called by Store in order to evaluate RAGA metrics\n",
    "#### This function ensure that you will use the right retriever to relevant to store i e strategy\n",
    "#### It also sends traces to langefuse for tracability for latency calculation \n",
    "#### Questions and Ground_truths are used from other AWS github examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58470f17-5864-4cd3-8546-20b8e34c30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatewithReriever (retriever_name,i):\n",
    "    questions = [\"How many days has Amazon asked employees to come to work in office?\", \n",
    "                 \"By what percentage did AWS revenue grow year-over-year in 2022?\",\n",
    "                 \"Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\",\n",
    "                 \"Which was the first inference chip launched by AWS according to the passage?\",\n",
    "                 \"According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\",\n",
    "                 \"What did the president say about Intel's CEO?\",\n",
    "                ]\n",
    "    ground_truths = [[\"Amazon has asked corporate employees to come back to office at least three days a week beginning May 2022.\"],\n",
    "                    [\"AWS had a 29% year-over-year ('YoY') revenue in 2022 on $62B revenue base.\"],\n",
    "                    [\"In 2022, AWS delivered their Graviton3 chips, providing 25% better performance than the Graviton2 processors.\"],\n",
    "                    [\"AWS launched their first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense.\"],\n",
    "                    [\"Amazon's annual revenue increased from $245B in 2019 to $434B in 2022.\"],\n",
    "                    [\"The president asked Congress to pass proven measures to reduce gun violence.\"]]\n",
    "    \n",
    "    answers = []\n",
    "    contexts = []\n",
    "    ragas_scores = []\n",
    "    #print (\"retriever_name\" ,retriever_name)\n",
    "    # Inference\n",
    "    for query in questions:\n",
    "      answers.append(rag_chain.invoke(query))\n",
    "      contexts.append([docs.page_content for docs in retriever_name.get_relevant_documents(query)])\n",
    " \n",
    "    answer = rag_chain.invoke(query)\n",
    "     \n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    # Convert dict to dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19b800-3c43-4fab-a535-466f1652ba05",
   "metadata": {},
   "source": [
    "#### Use Claude3 sonnet,  RAGA to evaluate context_recall,context_precision, answer_relevancy, \n",
    "#### Load all results in dataframes in order to plot results \n",
    "#### Execution time will take between 5 - 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff374d6-267a-403d-ba6f-a0ff08514706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGA tested on the 5 FAISS strors  \n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "import nest_asyncio  # CHECK NOTES\n",
    "df_vector= []\n",
    "# NOTES: Only used when running on a jupyter notebook, otherwise comment or remove this function.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "for i in [0,1,2,3,4]:\n",
    "    retriever = db_vector[i].as_retriever()\n",
    "    dataset = evaluatewithReriever(retriever,i)\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=metrics,\n",
    "        llm=llm,\n",
    "        embeddings=bedrock_embeddings,\n",
    "    )\n",
    "    df = result.to_pandas()\n",
    "    df_vector.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a2d0d-9623-4afe-a1c4-492d57a7c072",
   "metadata": {},
   "source": [
    "#### Plot results per strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6aea07-1141-4d3f-9b9d-cf2eb64bbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in [0,1,2,3,4]:\n",
    "    ax = df_vector[i].plot.bar(figsize=(10,3) , title=my_strategies[i])\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7cdae-5cc7-4afd-936a-8b326da710d9",
   "metadata": {},
   "source": [
    "### Plot Results as aggregated Score in one Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b102e-85ad-4ba7-a756-f245bc9ee979",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create average DF Per strategy \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "dict = {'strategy' :[0], 'context_recall':[0],'context_precision':[0] , 'answer_relevancy':[0] }\n",
    "averagedf=pd.DataFrame(dict)\n",
    "#averagedf= pd.DataFrame(columns = ['Strategy', 'context_recall','context_precision' , 'answer_relevancy' ])\n",
    "for i in [0,1,2,3,4]:\n",
    "   df2 = {'strategy': my_strategies [i],'context_recall': df_vector[i][\"context_recall\"].mean() ,  'context_precision': df_vector[i][\"context_precision\"].mean()\n",
    "          , 'answer_relevancy': df_vector[i][\"answer_relevancy\"].mean() }\n",
    "   averagedf = averagedf._append(df2, ignore_index = True)\n",
    "   \n",
    "averagedf  = averagedf.drop(averagedf [(averagedf.strategy == '0')].index)\n",
    "averagedf = averagedf.drop([0])\n",
    "print (averagedf)\n",
    "\n",
    "ax = averagedf.plot.bar(x='strategy' ,figsize=(20,10) , title= \"Chunking Strategy Evaluation\") \n",
    "for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "plt.show()\n",
    "ax2 = averagedf.plot('strategy' ,figsize=(20,10) , title= \"Chunking Strategy Evaluation\") \n",
    "for container in ax2.containers:\n",
    "        ax2.bar_label(container)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ddd827-43a2-44c5-a069-912bdef48493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
