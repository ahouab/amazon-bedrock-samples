{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b486b69d-6b68-411a-ac55-61e620d9daee",
   "metadata": {},
   "source": [
    "# Fine-tune LLama3 using ORPO and Importing into Bedrock: A Step-by-Step Instructional Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292df08-5903-4506-bedc-0e2c2bec6bc8",
   "metadata": {},
   "source": [
    "In this notebook we will walk through how to fine-tune a Llama-3 LLM on Amazon SageMaker using PyTorch FSDP and Flash Attention 2 including Q-LORA and PEFT. This notebook also explains using PEFT and merging the adapters. \n",
    "\n",
    "We will quantize the model as bf16 model. We use [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer) (SFT) for fine tuning the model. We will use Anthropic/Vicuna like Chat Template with User: and Assistant: roles to fine tune the model. We will use [HuggingFaceH4/no_robots] (https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset for fine tuning the model. This is a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. Using [FSDP](https://pytorch.org/docs/main/fsdp.html) and [Q-Lora](https://arxiv.org/abs/2305.14314) allows us to fine tune Llama-3 models on 2x consumer GPU's. FSDP enables sharding model parameters, optimizer states and gradients across data parallel workers. Q- LORA helps reduce the memmory usage for finetuning LLM while preserving full 16-bit task performance. For fine tuning in this notebook we use ml.g5.12xlarge as a SageMaker Training Job. \n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker) provides a fully managed service that enables build, train and deploy ML models at scale using tools like notebooks, debuggers, profilers, pipelines, MLOps, and more – all in one integrated development environment (IDE). [SageMaker Model Training](https://aws.amazon.com/sagemaker/train/) reduces the time and cost to train and tune machine learning (ML) models at scale without the need to manage infrastructure.\n",
    "\n",
    "For detailed instructions please refer to [Importing a model with customer model import Bedrock Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html).\n",
    "\n",
    "This notebook is inspired by Philipp Schmid Blog - https://www.philschmid.de/fsdp-qlora-llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf44813-7ad8-48a1-b544-1c5375032a10",
   "metadata": {},
   "source": [
    "### Install the Pre-Requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8cefe5-5c17-456f-9942-0fbd063a2124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./custom_import_venv/lib/python3.10/site-packages (24.1.2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m venv custom_import_venv\n",
    "source custom_import_venv/bin/activate\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install boto3 s3fs transformers \"aiobotocore\" \"sagemaker\" \"huggingface_hub\" \"datasets[s3]\" \"fsspec[http]\" \"botocore\" --upgrade --quiet\n",
    "# pip install boto3 s3fs transformers \"aiobotocore==2.11.0\" \"sagemaker>=2.190.0\" \"huggingface_hub\" \"datasets[s3]==2.18.0\" \"fsspec[http]<=2024.2.0\" \"botocore==1.33.2\" --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59470449-c81a-4c87-b302-353678358cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/nick_aws_samples/amazon-bedrock-samples/custom_models/import_models/llama-3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0256ca-321e-49c7-a7cf-cdbbcf633d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(f\"{os.getcwd()}/custom_import_venv/lib/python3.10/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6143de-aca6-4abb-8958-3d65c6d89405",
   "metadata": {},
   "source": [
    "Logging into the HuggingFace Hub and requesting access to the meta-llama/Meta-Llama-3-8B is required to download the model and finetune the same. Please follow the [HuggingFace User Token Documentation](https://huggingface.co/docs/hub/en/security-tokens) to request tokens to be provided in the textbox appearning below after you run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b58fab-d176-48ea-8169-21f7e512f6df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "    token=\"<token_here>",\n",
    "    add_to_git_credential=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f4c3c-2074-433f-859b-fad8ab00160c",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We will initialize the SageMaker Session required to finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2920f0e9-4ee7-4d8f-b803-72e5a4f4ad8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::425576326687:role/SageMakerStudioDomainNoAuth-SageMakerExecutionRole-3RBLN6GPZ46O\n",
      "sagemaker bucket: sagemaker-us-east-1-425576326687\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "    \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087ef13-2d35-41ac-b56c-25df22e26d85",
   "metadata": {},
   "source": [
    "### Define the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d273b4-6525-4214-a949-e929a6628e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# new_model = \"mccartni-orpo-llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c045100-b24b-4a00-8ab6-365589536721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/datasets/orpo-dpo-mix-40k'\n",
    "use_bf16 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36546e4-679e-44c4-933c-c0bfdebe3e1a",
   "metadata": {},
   "source": [
    "### Dataset Prepare\n",
    "We will use [mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k) dataset to finetune the Llama 3 model.\n",
    "\n",
    "We will transform the messages to OAI format and split the data into Train and Test set. The Train and Test dataset will be uploaded into S3 - SageMaker Session Bucket for use during finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80ca2437-4989-46ae-83a0-ddae617fc404",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'chosen', 'rejected', 'prompt', 'question'],\n",
      "        num_rows: 43802\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'chosen', 'rejected', 'prompt', 'question'],\n",
      "        num_rows: 443\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 44/44 [00:10<00:00,  4.10ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 28.76ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-425576326687/datasets/orpo-dpo-mix-40k/train_dataset.json\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-425576326687/?region=us-east-1&prefix=datasets/orpo-dpo-mix-40k/\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.01)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# save datasets to S3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e60545-6fe9-4389-b882-3f313e2bf71f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training script and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0344b7-4e2e-4b4d-bce5-19504d2118e4",
   "metadata": {},
   "source": [
    "Create the scripts directory to hold the training script and dependencies list. This directory will be provided to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56417c49-f672-41cd-af36-223fe2bf905c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"scripts/orpo-sft\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2e050-c4e2-4147-ac76-a4c95c8b0bee",
   "metadata": {},
   "source": [
    "Create the requirements file that will be used by the SageMaker Job container to initialize the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7999af55-cefa-47d1-98b7-bb60b34a20cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/orpo-sft/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/orpo-sft/requirements.txt\n",
    "torch==2.2.2\n",
    "transformers==4.40.2\n",
    "sagemaker>=2.190.0\n",
    "datasets==2.18.0\n",
    "accelerate==0.29.3\n",
    "evaluate==0.4.1\n",
    "bitsandbytes==0.43.1\n",
    "trl==0.8.6\n",
    "peft==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2dd15-f28c-42ca-8519-957b2c88e8f6",
   "metadata": {},
   "source": [
    "Training Script that will use PyTorch FSDP, QLORA, PEFT and train the model using SFT Trainer. This script also includes prepping the data to Llama 3 chat template (Anthropic/Vicuna format). This training script is being written to the scripts folder along with the requirements file that will be used by the SageMaker Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f28d7ad-f6e0-44b3-bec8-3e15e4748885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/orpo-sft/run_orpo_qlora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/orpo-sft/run_orpo_qlora.py\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.system(\"pip install flash-attn --no-build-isolation --upgrade\")\n",
    "except:\n",
    "    print(\"flash-attn failed to install\")\n",
    "    \n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "\n",
    "# LLAMA_3_CHAT_TEMPLATE = (\n",
    "#     \"{% for message in messages %}\"\n",
    "#         \"{% if message['role'] == 'system' %}\"\n",
    "#             \"{{ message['content'] }}\"\n",
    "#         \"{% elif message['role'] == 'user' %}\"\n",
    "#             \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "#         \"{% elif message['role'] == 'assistant' %}\"\n",
    "#             \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "#         \"{% endif %}\"\n",
    "#     \"{% endfor %}\"\n",
    "#     \"{% if add_generation_prompt %}\"\n",
    "#     \"{{ '\\n\\nAssistant: ' }}\"\n",
    "#     \"{% endif %}\"\n",
    "# )\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to the dataset\"\n",
    "        },\n",
    "    )\n",
    "    model_id: str = field(\n",
    "        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n",
    "    )\n",
    "    use_qlora: bool = field(default=False, metadata={\"help\": \"Whether to use QLORA\"})\n",
    "    merge_adapters: bool = field(\n",
    "        metadata={\"help\": \"Wether to merge weights for LoRA.\"},\n",
    "        default=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "def training_function(script_args, training_args):\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    \n",
    "    train_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "    test_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "    \n",
    "    ################\n",
    "    # Model & Tokenizer\n",
    "    ################\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    # tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "    \n",
    "    def format_chat_template(row):\n",
    "        row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "        row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "        return row\n",
    "\n",
    "    train_dataset = train_dataset.map(format_chat_template)\n",
    "    test_dataset = test_dataset.map(format_chat_template)\n",
    "    \n",
    "    # print random sample\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"Log a few random samples from the processed training set\"\n",
    "    ):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(train_dataset[index][\"text\"])\n",
    "    \n",
    "    # Model    \n",
    "    torch_dtype = torch.bfloat16 if training_args.bf16 else torch.float32\n",
    "    quant_storage_dtype = torch.bfloat16\n",
    "    \n",
    "    # QLoRA config\n",
    "    if script_args.use_qlora:\n",
    "        print(f\"Using QLoRA - {torch_dtype}\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_quant_storage=quant_storage_dtype,\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        script_args.model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        torch_dtype=quant_storage_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    # PEFT\n",
    "    ################\n",
    "\n",
    "    # LoRA config\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    \n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "\n",
    "    orpo_args = ORPOConfig(\n",
    "        learning_rate=8e-6,\n",
    "        beta=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        max_length=1024,\n",
    "        max_prompt_length=512,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        report_to=\"wandb\",\n",
    "        output_dir=\"./results/\",\n",
    "    )\n",
    "\n",
    "    trainer = ORPOTrainer(\n",
    "        model=model,\n",
    "        args=orpo_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.model.print_trainable_parameters()\n",
    "    \n",
    "    ##########################\n",
    "    # Train model\n",
    "    #################\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "        \n",
    "    training_results = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    print(\"Training completed:\", training_results)\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Evaluation results:\", eval_results)\n",
    "    \n",
    "    ##########################\n",
    "    # SAVE MODEL FOR SAGEMAKER\n",
    "    ##########################\n",
    "    sagemaker_save_dir = \"/opt/ml/model\"\n",
    "\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "    trainer.save_model(sagemaker_save_dir)\n",
    "    \n",
    "    if script_args.merge_adapters:\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int 4 model\n",
    "        print('########## Merging Adapters  ##########')\n",
    "        trainer.model.save_pretrained(training_args.output_dir)\n",
    "        trainer.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        # list file in output_dir\n",
    "        print(os.listdir(training_args.output_dir))\n",
    "\n",
    "        # load PEFT model\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            training_args.output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch_dtype\n",
    "        )\n",
    "        # Merge LoRA and base model and save\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(\n",
    "            sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        trainer.model.save_pretrained(sagemaker_save_dir, safe_serialization=True)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = HfArgumentParser((ScriptArguments, TrainingArguments))\n",
    "    script_args, training_args = parser.parse_args_into_dataclasses()    \n",
    "    \n",
    "    # set use reentrant to False\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "    # set seed\n",
    "    set_seed(training_args.seed)\n",
    "  \n",
    "    # launch training\n",
    "    training_function(script_args, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a951f-61b9-4cae-afb6-fb39db0f563c",
   "metadata": {},
   "source": [
    "Hyperparameters, which are passed into the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f30a0a-ad48-445a-b696-075e3e7ad542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/',    # path where sagemaker will save training dataset\n",
    "  'model_id': base_model,                              # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 1024,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 1,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                  # batch size per device during training\n",
    "  'per_device_eval_batch_size': 2,                   # batch size for evaluation    \n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"paged_adamw_8bit\",                            # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'evaluation_strategy': \"steps\",\n",
    "  'learning_rate': 8e-6,                           # learning rate, based on QLoRA paper\n",
    "  'bf16': use_bf16,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "  'fsdp': '\"full_shard auto_wrap offload\"',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06584a9-48e5-46bd-827b-4a13c31376b7",
   "metadata": {},
   "source": [
    "Use the SageMaker HuggingFace Estimator to finetune the model passing in the hyperparameters and the scripts directory from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fba89db8-7930-42c5-a25f-9d0bb12c69a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9fbb6-99c1-4e35-be42-b26bb6548eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: meta-llama-Meta-Llama-3-8B-bf16-orpo-20-2024-07-10-12-01-54-959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 12:01:55 Starting - Starting the training job...\n",
      "2024-07-10 12:01:56 Pending - Training job waiting for capacity................................................................................"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder \n",
    "import time\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'{base_model.replace(\"/\", \"-\")}-{\"bf16\" if use_bf16 else \"f32\" }-{\"orpo\"}-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    " \n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_orpo_qlora.py',    # train script\n",
    "    source_dir           = 'scripts/orpo-sft/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment          = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # Retrieve HuggingFace Token to be used for downloading base models from\n",
    "        \"ACCELERATE_USE_FSDP\":\"1\", \n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\":\"1\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a73bd-973f-44e6-9125-aefb69496f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e1df4-0af5-4fa3-a810-77a7c36e9b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448790c-65bf-4be9-bb23-799e6245d6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa58852-66ef-4bc3-a57a-768becb359e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa8a9f-459d-49ab-9113-218e1472f6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1f247-9e91-4248-82fd-4d7d03c7297c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b8e91-3863-4c14-b7fd-869376768853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c98d355-2df0-4e85-ab08-672aed5b32eb",
   "metadata": {},
   "source": [
    "### Log in to HuggingFace (to ensure we can read in LLama3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab92fda-3aee-4a1e-bf75-90251b44eced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "    token=\"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\",\n",
    "    add_to_git_credential=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0c126-85b2-48a0-aa62-f1222d1353d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "new_model = \"mccartni-orpo-llama-3-8B\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f71904-69c2-47dc-8970-f006bfebe707",
   "metadata": {},
   "source": [
    "### Prepare ORPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e439bf-0876-421f-aeef-68be36630152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdc5be-a410-482c-a2d3-58603bf80235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(10000))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= os.cpu_count(),\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.01)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1c6bc-710a-4b36-b0f8-b5c211cf3674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\",\n",
    ")\n",
    "\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "training_results = trainer.train()\n",
    "print(\"Training completed:\", training_results)\n",
    "\n",
    "trainer.save_model(new_model)\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc026e-3c5d-4af0-a9cc-cb70fc6b5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(\"13dafa51b3dc2f45ebc7a6c217cc74c4b3974316\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e1f79-9986-49b7-91a9-4f1b2ace7778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training completed:\", training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b1c6b-a9a6-44f8-8f00-79fbacc262b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae212a-d2f9-45a7-b14c-3ef5cd28bdd7",
   "metadata": {},
   "source": [
    "### Test and qualitatively evaluate the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ce200-a931-40b6-ab51-9f3c2cf30948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example from the test dataset that one can use to compare the original result and \n",
    "\n",
    "example_message = [{'content': 'In this task, you are given the name of an Indian food dish. You need to classify the dish as a \"main course\", \"dessert\" or \"snack\".\\n\\nGheela Pitha\\ndessert\\n\\nDaal baati churma\\nmain course\\n\\nKhorisa\\n',\n",
    "  'role': 'user'},\n",
    " {'content': 'snack', 'role': 'assistant'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9262f-5eae-4914-8b4d-a8ec44de4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "eval_dataset = test_dataset\n",
    "# rand_idx = randint(0, len(eval_dataset))\n",
    "rand_idx = 35\n",
    "messages = eval_dataset[rand_idx][\"messages\"][:]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184cfa8-907c-4716-b916-c49d05d526cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test on sample\n",
    "input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=520,\n",
    "    eos_token_id= tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    ")\n",
    "full_response = outputs[0]\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "print(f\"**Generated Answer:**\\n{tokenizer.decode(response, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783543b-5224-44ec-9542-e1df58a129a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][0]['content']}\\n\")\n",
    "\n",
    "print(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "\n",
    "print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1452bf-32c0-41d4-9f7e-da64c33c314f",
   "metadata": {},
   "source": [
    "### Flush memory and merge adapter into the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d9ea4-5db5-4a91-9450-8e555275c3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n",
    "\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(fp16_model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ac57a-7bcd-4be8-993c-53652096c639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"bedrock_{new_model}\")\n",
    "tokenizer.save_pretrained(f\"bedrock_{new_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c406f-edca-4051-945a-54d81bd3a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6de9e-9bff-4312-9dd7-9b069e904399",
   "metadata": {},
   "source": [
    "#### Upload weights to S3 or downstream Bedrock custom import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937bbcb-3f65-49a8-851b-1526baec25b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_weights_s3_location = f\"s3://{output_bucket}/llama3-hf-modelweights\"\n",
    "S3Uploader.upload(f\"./bedrock_{new_model}/\", model_weights_s3_location)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.2.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.2.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
