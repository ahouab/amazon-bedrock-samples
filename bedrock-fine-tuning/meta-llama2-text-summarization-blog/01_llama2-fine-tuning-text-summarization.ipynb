{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> This module cannot be executed in Workshop Studio Accounts, and you will have to run this notebook in your own account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama2 Pre-trained Models using Amazon Bedrock\n",
    "\n",
    "Prompting Large Language Models (aka. LLM) to generate highly accurate and relevant responses on Natural Language Processing (NLP) tasks is a powerful way to build generative AI applications. To do so, you may begin with trying simple prompting techniques, such as zero-shot / few-shot prompting. Sooner or later, you may realize customizing LLMs on your specific use cases is an effective way to deliver great business outcomes; hence, you may experiment Retrieval Augmented Generation (RAG) to steer LLMs on context information for your tasks, especially, RAG suits most for question & answer task. Besides RAG, there is another alternative to custom LLMs - fine-tuning LLMs on custom data, and it suits for text summarization and classification tasks while improving model responses. In this blog, we will walk through fine-tuning Meta Llama2 13B pre-trained model on a text summarization task using Model Customization for Amazon Bedrock, which provides optimized performance on your specific use case(s) easily without in-depth ML expertise.\n",
    "\n",
    "### What is Llama2 Foundation Models?\n",
    "Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. Llama 2 is open-sourced and publicly available. It comes in a range of parameter sizes—7 billion, 13 billion, and 70 billion—as well as pre-trained and fine-tuned variations. According to Meta, the tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. Llama 2 was pre-trained on 2 trillion tokens of data from publicly available sources. The tuned models are intended for assistant-like chat, whereas pre-trained models can be adapted for a variety of natural language generation tasks. Regardless of which version of the model a developer uses, [the responsible use guide](https://llama.meta.com/responsible-use-guide/) from Meta can assist in guiding additional fine-tuning that may be necessary to customize and optimize the models with appropriate safety mitigations. Also, Amazon Bedrock currently supports Llama2 13 billion and 70 billion models, and customers need to check End-User License Agreement (EULA) before planning to use it for commercial activities.\n",
    "\n",
    "### What is Amazon Bedrock?\n",
    "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.\n",
    "\n",
    "To fine-tune Llama2 13B pretrained model in Model Customization for Amazon Bedrock, we will walk through the process, including data preparation, model fine-tuning, training process analysis, model deployment using [Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html) provided by Amazon Bedrock and evaluation using [BERTScore](https://arxiv.org/abs/1904.09675) metric. We will share the code on the key steps and will include AWS console guidance. To access the source code, please refer to code repository - [meta-llama2-fine-tuning-text-summarization](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/bedrock-fine-tuning/meta-llama2-text-summarization-blog).\n",
    "\n",
    "> Given Amazon Bedrock makes LLMs fine-tuning easily achievable, besides using Llama2 13B pre-trained model for fine-tuning, you may want to explore other foundation models for your specific use cases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For following up the below fine-tuning process, please run [00_setup.ipynb](./00\\_setup.ipynb) notebook to setup necessary S3 bucket or IAM access role. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup environment variable for default region and aws profile. \n",
    "\n",
    "> If you are running the notebook in an EC2 instance / SageMaker notebook environment, which providing necessary IAM permissions, please ignore `AWS_PROFILE` setup. Otherwise, please setup an AWS profile at your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_DEFAULT_REGION=us-east-1\n",
    "%env AWS_PROFILE="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket_name\n",
    "%store -r role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In our example, we are using open sourced [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) dataset for a text summarization task. [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) is a large-scale dialogue summarization dataset, including 14,460 dialogues with corresponding manually-labelled summaries and topics.  \n",
    "\n",
    "Before fine-tuning Llama2 models in Amazon Bedrock, we need to prepare a training and optional validation dataset by creating a JSONL file with multiple JSON lines. As we fine-tune a text-to-text model, each JSON line must contain a `prompt` and `completion` fields. While using [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) dataset, we will transform `summary` attribute to `prompt` field using a text summarization prompt template, and take `summary` attribute as `completion` field. (reference doc - [Prepare the datasets](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-prereq.html#model-customization-prepare))\n",
    "\n",
    "Below is the example of “fine-tuning: text-to-text” dataset format\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"<prompt1>\", \"completion\": \"<expected generated text>\"}\n",
    "{\"prompt\": \"<prompt2>\", \"completion\": \"<expected generated text>\"}\n",
    "...\n",
    "```\n",
    "\n",
    "And, below is an example for a text summarization task:\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"### Instruction\\n\\nSummarize the following conversation...\", \"completion\": \"Jessica tells Mr. White...\"}\n",
    "...\n",
    "```\n",
    "In general, the dataset size should be comparable to model size. The more data records you have in training dataset, the better the quality of the fine-tuned model. However, to simplify the data preparation, we will sample 1K records from [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) dataset. Please note that model customization job for Llama2 fine-tuning can support upto 10K records for training and 1K for validation dataset. Furthermore, we will split the data into training, validation and test for training and model evaluation.\n",
    "\n",
    "> Splitting training data to training, validation and test is one of the ML best practices. We will be using training and validation dataset in model customization job (for fine-tuning), which generates training process output for us to analyze how the fine-tuned model(s) fit on training data; then, we will use the test dataset as part of the evaluation. For learning more about the best practices, please refer to  [Training, validation, and test data sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s walk through the code for the data preparation process:\n",
    "\n",
    "To setup local data folder for storing [DialogSum dataset](https://huggingface.co/datasets/knkarthick/dialogsum) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# to create local data folder\n",
    "data_folder = Path.cwd() / \"data\"\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "source_file_path = data_folder / \"dialogsum.train.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the [DialogSum dataset file](https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.train.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "\n",
    "source_training_data_uri = \"https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.train.jsonl\"\n",
    "request.urlretrieve(source_training_data_uri, str(source_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose prompt, completion attributes and sample 1,000 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(source_file_path, lines=True)\n",
    "\n",
    "# renaming to \"prompt\" & \"completion\" to meet fine-tuning dataset format\n",
    "df.rename(columns={\"dialogue\": \"prompt\", \"summary\": \"completion\"}, inplace=True)\n",
    "\n",
    "# only keep the fine-tuning data fields\n",
    "df = df[[\"prompt\", \"completion\"]]\n",
    "\n",
    "# only sample 1,000 records for our fine-tuning example\n",
    "df = df.sample(n=1_000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data to training, validation & test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to be 80% training, 10% validation, 10% testing.\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "validation_df = df.drop(train_df.index)  \n",
    "test_df = validation_df.sample(frac=0.5, random_state=42)\n",
    "validation_df = validation_df.drop(test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define specified prompt template for fine-tuning training and validation dataset. Especially, you can define your prompt style and tone, which may work best for your specific use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_fine_tuning_prompt_template = \"\"\"\n",
    "### Instruction\n",
    "Summarize the following conversation.\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\n",
    "### Answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only apply the prompt template to training and validation dataset, which will be the input for model customization. We will leave the test dataset for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['prompt'] = train_df['prompt'].apply(lambda x: llama2_fine_tuning_prompt_template.format(context=x))\n",
    "validation_df['prompt'] = validation_df['prompt'].apply(lambda x: llama2_fine_tuning_prompt_template.format(context=x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload training & validation dataset to S3 bucket, which will be input for model customization. . (To find m Python function `upload_data_to_s3` implementation - [utils.upload_data_to_s3()]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import upload_data_to_s3\n",
    "\n",
    "fine_tuning_data_type = \"text-summarization\"\n",
    "prefix = \"llama2\"\n",
    "train_data_uri = upload_data_to_s3(train_df, data_folder, f\"{fine_tuning_data_type}-training-data.jsonl\", bucket_name, prefix)\n",
    "validation_data_uri = upload_data_to_s3(validation_df, data_folder, f\"{fine_tuning_data_type}-validation-data.jsonl\", bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama2 13B pre-trained model\n",
    "\n",
    "Fine-tuning LLMs is a process of providing training data to a model (pre-trained or fine-tuned foundation models) in order to improve its performance on specific use-cases, which creates better user experience on generative AI applications. Custom models for Amazon Bedrock provides APIs to make the process of fine-tuning LLMs easy and accessible; instead of creating your own training script, you only need to call APIs using selected base model, hyperparameters and training and evaluation dataset, etc. \n",
    "\n",
    "> To find out more base models which can be fine-tuned with Amazon Bedrock, please check boto3 API document - [list_foundation_models()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock/client/list_foundation_models.html).\n",
    "\n",
    "Besides high-quality dataset, using proper hyperparameters for training process is the key to achieve high performing fine-tuned model(s). Custom models for Amazon Bedrock supports 3 hyperparameters - epochCount, batchSize and learningRate. To learn more details, please refer to [Meta Llama 2 model customization hyperparameters document](https://docs.aws.amazon.com/bedrock/latest/userguide/cm-hp-meta-llama2.html). Meanwhile, to find out proper hyperparameters, it’s recommended to analyze model customization job results via plotting training and validation metrics from the [output files](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-analyze.html). For Llama2 model fine-tuning, Custom model for Amazon Bedrock provide ‘loss’ and ‘perplexity’ training metrics, and optional validation metrics when validation dataset is provided in model customization job. \n",
    "\n",
    "Now, we will submit model customization job(s) to kick off Llama2 13B pre-trained model fine-tuning. There are two options to create a job: use python boto3 SDK and AWS console:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Using Python boto3 API\n",
    "\n",
    "> In the below code, we are using a IAM role for a model customization job so as to access training and validation dataset, and generate the output to target S3 folder. For more detail on setting up the IAM role, please refer to [Set up a service role with permissions to run a model customization job](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "# the fine-tunable base model id in Bedrock\n",
    "base_model_id='meta.llama2-13b-v1:0:4k'\n",
    "\n",
    "custom_model_name = \"llama2-13b-textsum-custom-model-000\"\n",
    "model_customization_job_name = f\"{custom_model_name}-job\"\n",
    "\n",
    "fine_tuning_output_uri = f\"s3://{bucket_name}/{prefix}/output\"\n",
    "\n",
    "# kick off model customization job\n",
    "response = bedrock.create_model_customization_job(\n",
    "    jobName=model_customization_job_name,\n",
    "    customModelName=custom_model_name,\n",
    "    roleArn=role_arn, # the IAM role for Amazon Bedrock to read training / validation dataset and output training process to S3 bucket(s)\n",
    "    baseModelIdentifier=base_model_id,\n",
    "    customizationType=\"FINE_TUNING\",\n",
    "    hyperParameters={\n",
    "        \"epochCount\": \"4\",\n",
    "        \"batchSize\": \"1\",\n",
    "        \"learningRate\": \"0.0004\"\n",
    "    },\n",
    "    trainingDataConfig={\n",
    "        \"s3Uri\": train_data_uri  # training dataset S3 uri\n",
    "    },\n",
    "    validationDataConfig={\n",
    "        'validators': [\n",
    "            {\n",
    "                's3Uri': validation_data_uri  # validation dataset S3 uri\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": fine_tuning_output_uri  # output location for training process\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using AWS console\n",
    "\n",
    "You may login to your AWS account and navigate to **Amazon Bedrock**, then select **Custom models**.\n",
    "\n",
    "![fine-tuning-in-console](./images/Bedrock-custom-models-fine-tuning.png)\n",
    "\n",
    "To create a fine-tuning job, choose **Customize model**, then choose **Create Fine-tuning job**, and fill in fine-tuning job details:\n",
    "\n",
    "![create-fine-tuning-job](./images/Bedrock-custom-models-create-fine-tuning-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze model customization job results\n",
    "\n",
    "Once the job is completed, your fine-tuned model is stored securely by Amazon Bedrock. \n",
    "\n",
    "Before deploying / testing the model, it's highly recommended to analyze the training process to evaluate [overfitting](https://aws.amazon.com/what-is/overfitting/) or [underfitting](https://aws.amazon.com/what-is/overfitting/) risk. To do so, you may download the model customization output and analyze the training & evaluation process. (For more details - [Analyze job results](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-analyze.html))\n",
    "\n",
    "Amazon Bedrock custom models service doesn't visualize the training process. However, we can use `pandas` and `matplotlib` to plot the loss and perplexity result for training and evaluation process. Below code snippet provides a custom python function `plot_train_process` to access and plot the training process results. (For source code reference - [model_utils.plot_training_process()]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model_utils import (\n",
    "    plot_training_process, \n",
    ")\n",
    "\n",
    "training_job_name = model_customization_job_name\n",
    "plot_training_process(training_job_name, include_metrics=['loss', 'perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of **overfitting**, when fine-tuning a Llama2 pre-trained model with 10 epochs, the model learns too well on training data, and will not generalize well on other text summarization tasks, e.g. in the diagram, training loss continuously gets smaller after epoch 4, but the validation loss gets larger. To avoid **overfitting**, we may tune the hyperparameter `epochCount` to be 4, or may try different `learning_rate`. For more details on model customization, please refer to these [guidelines](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-guidelines.html).\n",
    "\n",
    "![overfitting](./images/Bedrock-fine-tuning-analysis-overfitting.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom model(s) deployment using Provisioned Throughput\n",
    "\n",
    "Custom model(s) deployment is managed with [Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html), which provides a level of throughput at a fixed cost. The cost varies depending on the model that you use and the level of commitment you choose. There are three types of commitments: `1-month`, `6-month`, or `no commitment` (hourly pricing). For demo purpose, we will use 'no commitment' option to deploy the fine-tuned Llama2 13B model, and please remember to delete the [Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html) resource when it's not being used.\n",
    "\n",
    "Now, we will purchase [Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html) for deploying Llama2 13B fine-tuned model. There are two options provided to illustrate how to use python boto3 SDK and UI to create a job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Using Python boto3 API\n",
    "\n",
    "> ‘commitmentDuration’ is ignored in the API call so as to create ‘no commitment’ Provisioned Throughput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=\"pt-llama2-custom-model\",\n",
    "    modelId=custom_model_name, # the custmoer model name you use in fine-tuning job\n",
    "    # commitmentDurgion parameter is ignored to set 'no commitment' option.\n",
    ")\n",
    "pt_arn = response['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Using AWS console\n",
    "\n",
    "You may login to your AWS account and navigate to **Amazon Bedrock**, then select **Provisioned Throughput**.\n",
    "\n",
    "![bedrock-custom-model-pt](./images/Bedrock-custom-models-pt-purchase.png)\n",
    "\n",
    "then, to choose **Purchase Provisioned Throughput**, and provide the details:\n",
    "\n",
    "![bedrock-custom-model-pt-creation](./images/Bedrock-custom-models-pt-creation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the provisioned throughput, we can use the test dataset to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs model evaluation\n",
    "\n",
    "Model evaluation is a critical step to understand how well the (fine-tuned) foundation models may fit to your use cases, and we will be focused on ‘Accuracy’ metric, which is to evaluate how accurately the LLM model output may match the reference summary (aka. ground truth). In general, there are two types of evaluation: human evaluation vs algorithm evaluation. Especially, for algorithm evaluation, it can be done via lexical metrics, e.g. [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) (Recall-Oriented Understudy Generation Evaluation), or semantic similarity, e.g. [BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore) metric.\n",
    "\n",
    "In the text summarization task, we will be measuring ‘Accuracy’ metric using F1 measure of [BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore) metrics. [BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore) metrics suit the task given it has been shown to correlate with human judgement on sentence-level and system-level evaluation.\n",
    "\n",
    "Now, let’s walk through the code on evaluating fine-tuned Llama2 13B pretrained. Furthermore, we will be doing the same on Amazon Bedrock base model - Llama2 13B Chat base model provided by Amazon Bedrock. Last, we will compare the F1 measure of [BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore) metrics between the two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make Amazon Bedrock model inferencing code reusable, we prepare a Python function `completion()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "def completion(\n",
    "        model_id:str, \n",
    "        prompt:str, \n",
    "        temperature:float=0.5, \n",
    "        top_p:float=0.9,\n",
    "        max_tokens:int=256):\n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_gen_len\": max_tokens\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        modelId=model_id, # OD Base model id or PT ARN\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    generation = response_body[\"generation\"]\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will choose a random record from test dataset and use it to compare the model output between the fine-tuned model and Llama2 13B Chat base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = test_df.sample(n=1, random_state=23).iloc[0]\n",
    "\n",
    "dialogue = row['prompt']\n",
    "reference_summary = row['completion']\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(dialogue)\n",
    "print(\"-\" * 20)\n",
    "print(reference_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get model output from the fine-tuned model using the defined prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = llama2_fine_tuning_prompt_template.format(context=dialogue)\n",
    "\n",
    "# pt_arn is the Provisioned Throughput ARN for the fine-tuned model\n",
    "ft_model_input = completion(pt_arn, prompt)\n",
    "\n",
    "print(ft_model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to get the model output from Llama2 13B Chat base model using on-demand invocation. Please note that Llama2 prompt tags are being used to better output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_chat_specific_prompt_template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant, which is good at reading dialogues and generate concise summary.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "Please read the below dialogue and provide a concise summary:\n",
    "\n",
    "[/INST]\n",
    "Dialogue:\n",
    "{context}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompt = llama2_chat_specific_prompt_template.format(context=dialogue)\n",
    "llama2_13b_chat_base_model_id = \"meta.llama2-13b-chat-v1\"\n",
    "llama2_chat_model_output = completion(llama2_13b_chat_base_model_id, prompt)\n",
    "\n",
    "print(llama2_chat_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the model output from both, we will evaluate based on F1 measure of BERTScore metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate as hf_evaluate\n",
    "\n",
    "hf_bertscore = hf_evaluate.load(\"bertscore\")\n",
    "\n",
    "def get_bert_score(reference_output: str, model_output: str, lang: str=\"en\", model_type: str=\"roberta-large\") -> float:\n",
    "    result = hf_bertscore.compute(\n",
    "        predictions=[model_output],\n",
    "        references=[reference_output], \n",
    "        lang=lang, \n",
    "        model_type=model_type\n",
    "    )\n",
    "    return result\n",
    "\n",
    "fine_tuned_bertscore = get_bert_score(reference_summary, ft_model_input)\n",
    "print(f\"Model output from fine-tuned Llama2 13B model - BERTScore: {fine_tuned_bertscore['f1'][0]}\")\n",
    "\n",
    "chat_bertscore = get_bert_score(reference_summary, llama2_chat_model_output)\n",
    "print(f\"Model output from Llama2 13B Chat model - BERTScore: {chat_bertscore['f1'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison result above indicates that fine-tuned Llama2 13B model performs better than Llama2 13B Chat model on text summarization for the selected data (BERTScore `0.9293031692504883` vs `0.8647099137306213`).\n",
    "\n",
    "> **Tip**: The prompt template we used for invoking Llama2 13B Chat model may not be the best, hence, you may do further prompt engineering to drive better result. Meanwhile, besides using BERTScore, you may consider using different metrics (e.g. ROUGE, or BLEU, etc.) for your use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In the blog, we discussed a text summarization task with fine-tuning Llama2 13B pre-trained model, and covered the key steps starting from data preparation, model customization, training process analysis, and custom model deployment in Bedrock. At the end, we evaluated fine-tuned model using semantic similarity metric - BERTScore, which provides a qualitative measurement on test dataset. Overall, you can leave the heavy-lifting engineering effort to Custom Models for Amazon Bedrock, and focus on data preparation, training process analysis while tuning hyper-parameters to achieve better fine-tuned model(s), and model evaluation before moving your fined-tuned model(s) to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "* [Amazon Bedrock Developer Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
